{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a few sentences from the wikipedia article\n",
    "# since wikipedia articles on Hollywood actors are all many pages long\n",
    "# I initially decided to use the Article extracts from the MediaWiki API as the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wiki_extract(article):    \n",
    "    response = requests.get('https://en.wikipedia.org/w/api.php',\n",
    "                            params={'action': 'query','format': 'json',\n",
    "                                    'titles': article,\n",
    "                                    'prop': 'extracts',\n",
    "                                    'exintro': False,\n",
    "                                    'explaintext': True,\n",
    "                                    'exsectionformat': 'plain'\n",
    "                                   }).json()\n",
    "    text = next(iter(response['query']['pages'].values()))['extract']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the wikipedia library to get the full content of the article\n",
    "# to build a more complex model\n",
    "import wikipedia\n",
    "def get_wiki_content(article):\n",
    "    return wikipedia.page(article).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all the raw wikipedia article extracts in a list\n",
    "#raw_wiki_extracts = []\n",
    "#for actor in actors:\n",
    "    #raw_wiki_extracts.append(get_wiki_extract(actor))\n",
    "#    raw_wiki_extracts.append(get_wiki_content(actor))\n",
    "#print(raw_wiki_extracts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaclyn_Betham\n",
      "Desi_Arnaz\n",
      "Kathy_Brier\n",
      "Ernie_Anderson\n",
      "Don_Ameche\n",
      "Candace_Cameron_Bure\n",
      "Chloe_Bennet\n",
      "Maude_Apatow\n",
      "Tammy_Blanchard\n",
      "Todd_Armstrong\n",
      "Devon_Aoki\n",
      "Fiona_Bishop\n",
      "Jack_Albertson\n",
      "Rosanna_Arquette\n",
      "Geoffrey_Arend\n",
      "David_Arkin\n",
      "Rodolfo_Acosta\n",
      "Zazie_Beetz\n",
      "Sam_Appel\n",
      "Paul_America\n",
      "Talia_Balsam\n",
      "Edward_Andrews\n",
      "Joe_Don_Baker\n",
      "Irving_Bacon\n",
      "Loanne_Bishop\n",
      "Tim_Allen\n",
      "Kevin_Bacon\n",
      "Lester_Allen\n",
      "Doris_Belack\n",
      "Anna_Belknap\n",
      "Tammy_Barr\n",
      "Courtney_Taylor_Burness\n",
      "Julie_Caitlin_Brown\n",
      "Crystal_Allen\n",
      "Jordana_Brewster\n",
      "Jason_Alexander\n",
      "Oscar_Apfel\n",
      "Hedy_Burress\n",
      "Gene_Autry\n",
      "Dana_Barron\n",
      "Michelle_Bauer\n",
      "Humbert_Allen_Astredo\n",
      "Delta_Burke\n",
      "Eion_Bailey\n",
      "Blythe_Auffarth\n",
      "Melanie_Amaro\n",
      "Susan_Blakely\n",
      "Brandy_Burre\n",
      "Darcy_Rose_Byrnes\n",
      "Rex_Allen\n",
      "Daniella_Alonso\n",
      "Bobby_Alto\n",
      "Alex_Borstein\n",
      "Olivia_Burnette\n",
      "Alma_Beltran\n",
      "Elizabeth_Alderfer\n",
      "Kristina_Anapau\n",
      "Allison_Balson\n",
      "Angela_Bassett\n",
      "Maxine_Bahns\n",
      "Nicole_Axelrod\n",
      "Shaindel_Antelis\n",
      "Billie_Joe_Armstrong\n",
      "Angelica_Bridges\n",
      "Rowan_Blanchard\n",
      "Lou_Antonio\n",
      "Bianca_Allen\n",
      "Emile_de_Antonio\n",
      "Edwin_August\n",
      "Mia_Barron\n",
      "Brandy_Aniston\n",
      "Bonnie_Aarons\n",
      "Jon_Abrahams\n",
      "Jessica_Biel\n",
      "Anne_Bancroft\n",
      "Lori_Alan\n",
      "Tori_Black\n",
      "Judith_Baldwin\n",
      "Mackenzie_Aladjem\n",
      "Nina_Arianda\n",
      "Trevor_Bardette\n",
      "Mika_Boorem\n",
      "Leslie_Bibb\n",
      "Anne_Bobby\n",
      "Rachel_Bilson\n",
      "Seth_Adkins\n",
      "Tia_Ballard\n",
      "Lucie_Arnaz\n",
      "Tessa_Albertson\n",
      "Ben_Bard\n",
      "Christopher_Allport\n",
      "Andrea_Barber\n",
      "Philip_Ahn\n",
      "Andrea_Anders\n",
      "DenÃ©e_Benton\n",
      "Ad-Rock\n",
      "Sophia_Taylor_Ali\n",
      "Susan_Anspach\n",
      "Allisyn_Ashley_Arm\n",
      "Eddie_Albert\n",
      "Heather_Burns\n",
      "Paget_Brewster\n",
      "Rick_Aviles\n",
      "Vanessa_Bayer\n",
      "Sharon_Blynn\n",
      "Antony_Alda\n",
      "Buddy_Baer\n",
      "Michael_Bacall\n",
      "Adria_Arjona\n",
      "Yancy_Butler\n",
      "Sarah_Uriarte_Berry\n",
      "Camille_Anderson\n",
      "Erville_Alderson\n",
      "Jenna_Boyd\n",
      "Lewis_Arquette\n",
      "Joseph_Attles\n",
      "Jordan_Bayne\n",
      "Danny_Aiello\n",
      "Joan_Allen\n",
      "Brec_Bassinger\n",
      "Byrdie_Bell\n",
      "Franklin_Adreon\n",
      "Diana_Barrows\n",
      "Rachel_Ames\n",
      "Pepper_Binkley\n",
      "Steffani_Brass\n",
      "Jasmin_Savoy_Brown\n",
      "Joy_Behar\n",
      "Elizabeth_Bogush\n",
      "Ashley_Bashioum\n",
      "Margaret_Avery\n",
      "Verna_Bloom\n",
      "Sebastian_Arcelus\n",
      "Jaylen_Barron\n",
      "Bryn_Apprill\n",
      "Avner_the_Eccentric\n",
      "Wesley_Addy\n",
      "Harry_Anderson\n",
      "Jack_Aranson\n",
      "Olivia_Barash\n",
      "Betsy_Brandt\n",
      "Alexandra_Breckenridge\n",
      "Greg_Ayres\n",
      "Agnes_Bruckner\n",
      "Morris_Ankrum\n",
      "Brent_Armitage\n",
      "Shauna_Bloom\n",
      "Conrad_Bain\n",
      "Shelbie_Bruce\n",
      "Abigail_Breslin\n"
     ]
    }
   ],
   "source": [
    "# Generate a random subset of 50 actors\n",
    "# with shuf -n 150 actors.txt > actors50.txt\n",
    "\n",
    "# this will fetch 150 wikipedia articles to build the corpus \n",
    "raw_wiki_extracts = []\n",
    "with open('actors150.txt') as actors_list:\n",
    "    for actor in actors_list:\n",
    "        actor = actor.strip()\n",
    "        content = get_wiki_content(actor)\n",
    "        print(actor)\n",
    "        raw_wiki_extracts.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def raw_to_clean_bag_of_words(text):\n",
    "    # Build a dict to translate TreeBank POS type to WordNet POS\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # keeps only whole words with at least four chars \n",
    "    tokenizer = RegexpTokenizer('\\w{4,}')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # wordnet lemmatizer reduces to word root forms\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    bag = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "        bag.append(lemma)\n",
    "    \n",
    "    # filter stopwords\n",
    "    bag = filter(lambda word: word not in stopwords.words('english'), bag)\n",
    "    return bag\n",
    "\n",
    "# Build a bag of words representation for each article\n",
    "bag = []\n",
    "for article in raw_wiki_extracts:\n",
    "    bag.append(raw_to_clean_bag_of_words(article))\n",
    "#print(bag[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in bag:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "bag = [[token for token in text if frequency[token] > 1] for text in bag]\n",
    "#print(bag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dictionary from all the articles\n",
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(bag)\n",
    "corpus = [dictionary.doc2bow(text) for text in bag]\n",
    "\n",
    "# Save the dictionary so we don't hammer wikipedia \n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  u'0.017*\"arnaz\" + 0.015*\"autry\" + 0.009*\"film\" + 0.007*\"television\" + 0.007*\"show\" + 0.007*\"lucy\"'),\n",
      " (1,\n",
      "  u'0.013*\"armstrong\" + 0.009*\"bacon\" + 0.008*\"film\" + 0.008*\"show\" + 0.007*\"behar\" + 0.006*\"release\"'),\n",
      " (2,\n",
      "  u'0.016*\"allen\" + 0.011*\"film\" + 0.010*\"role\" + 0.009*\"play\" + 0.007*\"broadway\" + 0.007*\"star\"'),\n",
      " (3,\n",
      "  u'0.020*\"film\" + 0.010*\"role\" + 0.010*\"star\" + 0.010*\"television\" + 0.008*\"series\" + 0.008*\"appear\"'),\n",
      " (4,\n",
      "  u'0.018*\"film\" + 0.013*\"role\" + 0.011*\"series\" + 0.009*\"play\" + 0.009*\"episode\" + 0.009*\"bassett\"')]\n"
     ]
    }
   ],
   "source": [
    "# Build a Latent Dirichlet Allocation (LDA) model with gensim\n",
    "# https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, \n",
    "                                           num_topics = 5, \n",
    "                                           id2word=dictionary, \n",
    "                                           update_every=0,\n",
    "                                           passes=20)\n",
    "ldamodel.save('actor_lda_model.gensim')\n",
    "topics = ldamodel.print_topics(num_words=6)\n",
    "pprint(topics)\n",
    "\n",
    "#from the model probabilities, it appears that there are no significant \n",
    "#high probabilty, unique words that separate the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actors = ['Brie_Larson','Jennifer_Aniston','Brad_Pitt','Angelina_Jolie',\n",
    "         'Winona_Ryder','Brittany_Murphy','Julia_Roberts', 'Kathy_Bates',\n",
    "         'Sarah_Paulson','Susan_Sarandon','Dustin_Hoffman','Robin_Williams',\n",
    "         'Alan_Alda','Judd_Apatow','Charlie_Adler','Kevin_Bacon','James_Arness',\n",
    "         'Will_Arnett','Steve_Bannos','Jonathan_Banks','Rick_Aviles','Bea_Arthur','Lauren_Bacall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Brie_Larson', [(1, 0.051107556), (3, 0.4656969), (4, 0.47781518)])\n",
      "('Jennifer_Aniston', [(1, 0.05110852), (3, 0.46568725), (4, 0.47782382)])\n",
      "('Brad_Pitt', [(1, 0.051108066), (3, 0.46568987), (4, 0.47782165)])\n",
      "('Angelina_Jolie', [(1, 0.05110684), (3, 0.46570358), (4, 0.47780916)])\n",
      "('Winona_Ryder', [(1, 0.051106825), (3, 0.46570373), (4, 0.4778091)])\n",
      "('Brittany_Murphy', [(1, 0.05110762), (3, 0.46569437), (4, 0.4778176)])\n",
      "('Julia_Roberts', [(1, 0.051109333), (3, 0.46568048), (4, 0.47782984)])\n",
      "('Kathy_Bates', [(1, 0.051107936), (3, 0.4656901), (4, 0.4778216)])\n",
      "('Sarah_Paulson', [(1, 0.05110715), (3, 0.46570075), (4, 0.47781166)])\n",
      "('Susan_Sarandon', [(1, 0.051108398), (3, 0.46569037), (4, 0.4778208)])\n",
      "('Dustin_Hoffman', [(1, 0.05110834), (3, 0.46568844), (4, 0.47782284)])\n",
      "('Robin_Williams', [(1, 0.05110863), (3, 0.46568555), (4, 0.47782546)])\n",
      "('Alan_Alda', [(1, 0.051107652), (3, 0.4656984), (4, 0.47781357)])\n",
      "('Judd_Apatow', [(1, 0.051106784), (3, 0.46570447), (4, 0.47780836)])\n",
      "('Charlie_Adler', [(1, 0.051108234), (3, 0.46568796), (4, 0.4778234)])\n",
      "('Kevin_Bacon', [(1, 0.051106945), (3, 0.4657012), (4, 0.47781146)])\n",
      "('James_Arness', [(1, 0.051106963), (3, 0.46570277), (4, 0.47780985)])\n",
      "('Will_Arnett', [(1, 0.051107336), (3, 0.46569926), (4, 0.477813)])\n",
      "('Steve_Bannos', [(1, 0.051109117), (3, 0.4656805), (4, 0.47783)])\n",
      "('Jonathan_Banks', [(1, 0.05110922), (3, 0.46568197), (4, 0.4778284)])\n",
      "('Rick_Aviles', [(1, 0.05110765), (3, 0.465695), (4, 0.477817)])\n",
      "('Bea_Arthur', [(1, 0.051108476), (3, 0.4656901), (4, 0.477821)])\n",
      "('Lauren_Bacall', [(1, 0.051108833), (3, 0.4656859), (4, 0.47782487)])\n"
     ]
    }
   ],
   "source": [
    "# Try to classify new actors with LDA model\n",
    "for actor in test_actors:\n",
    "    bow = dictionary.doc2bow(raw_to_clean_bag_of_words(get_wiki_content(test_actor)))\n",
    "    print(actor, ldamodel.get_document_topics(bow, minimum_probability=0.05 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.010*film + 0.006*play + 0.005*bacon + 0.005*award + 0.005*biel + 0.004*role'),\n",
       " (1,\n",
       "  u'0.007*arnaz + 0.006*ameche + 0.006*albert + 0.006*star + 0.006*film + 0.005*show'),\n",
       " (2,\n",
       "  u'0.006*episode + 0.005*film + 0.004*bilson + 0.003*breslin + 0.003*series + 0.003*play'),\n",
       " (3,\n",
       "  u'0.017*bassett + 0.007*film + 0.005*role + 0.003*play + 0.003*appear + 0.002*year'),\n",
       " (4,\n",
       "  u'0.012*autry + 0.004*gene + 0.004*film + 0.004*ranch + 0.003*allen + 0.003*also')]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the Hierarchical Dirichlet Process HDP model\n",
    "hdpmodel = gensim.models.HdpModel(corpus, id2word=dictionary)\n",
    "hdpmodel.optimal_ordering()\n",
    "hdpmodel.print_topics(num_topics=5, num_words=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Brie_Larson', -564.9070615803536)\n",
      "('Jennifer_Aniston', -564.9070615803536)\n",
      "('Brad_Pitt', -564.9070615803536)\n",
      "('Angelina_Jolie', -564.9070615803536)\n",
      "('Winona_Ryder', -564.9070615803536)\n",
      "('Brittany_Murphy', -564.9070615803536)\n",
      "('Julia_Roberts', -564.9070615803536)\n",
      "('Kathy_Bates', -564.9070615803536)\n",
      "('Sarah_Paulson', -564.9070615803536)\n",
      "('Susan_Sarandon', -564.9070615803536)\n",
      "('Dustin_Hoffman', -564.9070615803536)\n",
      "('Robin_Williams', -564.9070615803536)\n",
      "('Alan_Alda', -564.9070615803536)\n",
      "('Judd_Apatow', -564.9070615803536)\n",
      "('Charlie_Adler', -564.9070615803536)\n",
      "('Kevin_Bacon', -564.9070615803536)\n",
      "('James_Arness', -564.9070615803536)\n",
      "('Will_Arnett', -564.9070615803536)\n",
      "('Steve_Bannos', -564.9070615803536)\n",
      "('Jonathan_Banks', -564.9070615803536)\n",
      "('Rick_Aviles', -564.9070615803536)\n",
      "('Bea_Arthur', -564.9070615803536)\n",
      "('Lauren_Bacall', -564.9070615803536)\n"
     ]
    }
   ],
   "source": [
    "# Try to classify new actors with HDP model\n",
    "# https://radimrehurek.com/gensim/models/hdpmodel.html\n",
    "for actor in test_actors:\n",
    "    bow = [dictionary.doc2bow(raw_to_clean_bag_of_words(get_wiki_content(test_actor)))]\n",
    "    print(actor, hdpmodel.evaluate_test_corpus(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not clear to me how I can classify random wikipedia articles for hollywood actors by a specific theme. There does not appear to be sufficient distinguishing features from keywords alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
